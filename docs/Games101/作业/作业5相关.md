[Games101：作业5解析_games101作业5-CSDN博客](https://blog.csdn.net/Q_pril/article/details/123825665)

## 作业要求
1. **Renderer.cpp** **中的** **Render()**：这里你需要为每个像素生成一条对应的光线，然后调用函数 castRay() 来得到颜色，最后将颜色存储在帧缓冲区的相应像素中。
	1. 【难点】需要从scene中找到当前xy的坐标。
	2. 需要把坐标转化为屏幕坐标。
	3. 使用castRay函数得到颜色。
2. Triangle.hpp 中的 rayTriangleIntersect() : v0, v1, v2 是三角形的三个顶点， orig 是光线的起点， dir 是光线单位化的方向向量。 tnear, u, v 是你需要使用我们课上推导的 Moller-Trumbore 算法来更新的参数。
	1. 具体参考[[Lecture - 13 Ray tracing 光线追踪 技术难题#Moller-Trumbore 算法：和三角形相交]]
	2. 其实只需要带入公式即可(但是由于我完全忘了课上提到了这个算法所以不知道怎么做)

## 难点解析
### 整个渲染过程
参考：[Generating Camera Rays with Ray-Tracing](https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-generating-camera-rays/generating-camera-rays.html)
#### 眼射线
回顾一下光线追踪的原理：
1. 眼睛和摄像机重合，在原点（0，0，0）
2. 眼睛射出光线（下面被称为eye-tracing）
3. 光线和实际物体相交，则像素平面渲染此点
4. 否则不渲染此点。
>**backward** or **eye-tracing** (because we follow the path of light rays from the camera to the object and from the object to the source

![](https://www.scratchapixel.com/images/ray-tracing-camera/campixel.gif)

#### 为帧（frame）的每个像素创建一个主射线
我们的任务包括为帧的每个像素创建一个主射线。
1. 这可以通过跟踪一条从相机原点开始并穿过每个像素中间的线来轻松完成（图1）。
2. 我们可以用射线的形式表示这条线，其原点是相机原点，方向是从相机原点到像素中心的矢量。$r(t)=o+td$
3. 我们需要将最初在**栅格空间**（raster space）中表示的像素坐标（点坐标以像素表示，坐标（0,0）是帧的左上角）转换为**世界空间**（world space）。
#### 从栅格空间到世界空间
我们已知
1. 【相机位置】我们知道图像平面正好位于离世界原点一个单位的地方，并且沿着负z轴对齐。
2. 【正交投影】我们还知道图像是正方形的，这个投影区域的尺寸是2 × 2单位（图2）。[[Lecture - 3 Transformation#Projection transformation（投影）]]
下面需要三个步骤：
1. 从raster space 到NDC space
2. 从NDC space 到 screen space
3. 从screen space 到 world space
![image.png](https://gitee.com/dontt/picgo-img-bed/raw/master/img/20241108163332.png)

#### Raster to NDC
**NDC space** (which stands for Normalized Device Coordinates)
只需要把像素的中心除以长和宽即可。
$$
\begin{array}{l}
PixelNDC_x = \dfrac{(Pixel_x + 0.5)}{ImageWidth},\\
PixelNDC_y = \dfrac{(Pixel_y + 0.5)}{ImageHeight}.
\end{array}
$$
请注意，我们在像素位置上添加了一个小的偏移（0.5），因为我们希望最终的相机光线通过像素的中间。
在NDC空间中表示的像素坐标在$[0,1]$范围内（是的，光线追踪中的NDC空间与光栅化世界中的NDC空间不同，后者通常映射到$[-1,1]$范围）。
所以最后更正一下归一化为$[-1,1]$：
$$
\begin{array}{l}
PixelScreen_x = 2 * {PixelNDC_x} - 1,\\
PixelScreen_y = 2 * {PixelNDC_y} - 1.
\end{array}
$$
并且再次注意，y为正的时候x其实是为负的，所以应该反向。
$$
PixelScreen_y = 1 - 2 * {PixelNDC_y}.
$$
#### 宽高比 得到屏幕空间（screen space）
[[Lecture - 5 and 6 Rasteriazation (Triangles)#定义屏幕空间(screen space)]]
现在的假设都是基于正方形的，如果要变成长方形的，并且保持像素是正方形的形状，则需要和宽高比相乘。
该值现在从1到-1变化为从0到${ImageWidth}$。以这种方式表示的坐标被称为在屏幕空间（screen space）中定义。
![image.png](https://gitee.com/dontt/picgo-img-bed/raw/master/img/20241108164140.png)
$$\begin{array}{l}
ImageAspectRatio = \dfrac{ImageWidth}{ImageHeight},\\
PixelCamera_x = (2 * {PixelScreen_x} - 1) * {ImageAspectRatio},\\
PixelCamera_y = (1 - 2 * {PixelScreen_y}).
\end{array}$$
#### 视角
注意，到目前为止，屏幕空间中定义的任何点的y坐标都在$[- 1,1]$范围内。我们还知道接收平面离摄像机原点有1个单位的距离。如果我们从侧视图看相机设置，我们可以用一些简单的三角函数来求直角三角形ABC的角，它是对顶角的一半（alpha）.
![image.png](https://gitee.com/dontt/picgo-img-bed/raw/master/img/20241108164656.png)
$${\alpha \over 2} = atan (\dfrac{OppositeSide}{AdjacentSide }) = atan(\dfrac{1}{1}) = \dfrac{\pi}{4}.$$
得到角度是多少。
因此，我们可以将**屏幕像素坐标（screen pixel coordinate）**（目前包含在$[- 1,1]$范围内）乘以这个数字来放大或缩小它们。你可能已经猜到了，这个操作改变了我们看到的场景的多少，相当于放大（当视野缩小时我们看到的场景更少）和缩小（当视野增大时看到的场景更多）。
$$\||BC|| = tan(\dfrac{\alpha}{2}).$$
综上所述，我们可以用角度来定义相机的视场，并将屏幕像素坐标与该角度的正切值除以2的结果相乘（如果该角度以度表示，请不要忘记将其转换为弧度）

![image.png](https://gitee.com/dontt/picgo-img-bed/raw/master/img/20241108170058.png)

## 代码
```cpp
void render(
    const Options &options,
    const std::vector<std::unique_ptr<Object>> &objects,
    const std::vector<std::unique_ptr<Light>> &lights)
{
    Matrix44f cameraToWorld;
    Vec3f *framebuffer = new Vec3f[options.width * options.height];
    Vec3f *pix = framebuffer;
    float scale = tan(deg2rad(options.fov * 0.5));
    float imageAspectRatio = options.width / (float)options.height;
    Vec3f orig;
    cameraToWorld.multVecMatrix(Vec3f(0), orig);
    for (uint32_t j = 0; j < options.height; ++j) {
        for (uint32_t i = 0; i < options.width; ++i) {
            float x = (2 * (i + 0.5) / (float)options.width) * imageAspectRatio * scale;
            float y = (1 - 2 * (j + 0.5) / (float)options.height) * scale;
            Vec3f dir;
            cameraToWorld.multDirMatrix(Vec3f(x, y, -1), dir);
            dir.normalize();
            *(pix++) = castRay(orig, dir, objects, lights, options, 0);
        }
    }

    // Save result to a PPM image (keep these flags if you compile under Windows)
    std::ofstream ofs("./out.ppm", std::ios::out | std::ios::binary);
    ofs << "P6\n" << options.width << " " << options.height << "\n255\n";
    for (uint32_t i = 0; i < options.height * options.width; ++i) {
        char r = (char)(255 * clamp(0, 1, framebuffer[i].x));
        char g = (char)(255 * clamp(0, 1, framebuffer[i].y));
        char b = (char)(255 * clamp(0, 1, framebuffer[i].z));
        ofs << r << g << b;
    }

    ofs.close();

    delete [] framebuffer;
}
```